{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Lab: Predicting diabetes with a Simple Neural Network\n",
        "\n",
        "In this exercise you'll complete a **training loop** for a simple neural network built using **NumPy**.\n",
        "\n",
        "**Learning Objective:**  \n",
        "By the end of this lab you'll understand how to implement a forward pass, compute binary cross-entropy loss, perform backpropagation and update weights using gradient descent all from scratch.\n",
        "\n",
        "**Your goal:**\n",
        "Teach the network to predict whether a patient has **diabetes (1)** or **not (0)** based on 3 clinical inputs: **BMI, Age, Glucose**.\n",
        "\n",
        "## What you'll implement:\n",
        "- Forward pass,\n",
        "- Binary cross-entropy loss,\n",
        "- Backpropagation,\n",
        "- Weight updates using gradient descent."
      ],
      "metadata": {
        "id": "leFo75It5p7i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset and setup\n",
        "\n",
        "Let's generate a synthetic dataset of 100 patients, each with 3 features (BMI, Age, Glucose). We'll also normalize the features for better training stability.\n",
        "\n",
        "**Why normalize?** Neural networks train better when input features have similar scales (mean=0, std=1)."
      ],
      "metadata": {
        "id": "4tooGl-o38iF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T9T3xFnV3U1n"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "np.random.seed(42)\n",
        "\n",
        "# Simulated input data: 100 patients, 3 features (BMI, Age, Glucose)\n",
        "X = np.random.randn(100, 3)\n",
        "true_weights = np.array([[2], [-1], [1.5]])\n",
        "y_true = (1 / (1 + np.exp(-X @ true_weights)) > 0.5).astype(int)\n",
        "\n",
        "# Normalize features (mean=0, std=1)\n",
        "X = (X - X.mean(axis=0)) / X.std(axis=0)\n",
        "\n",
        "print(f\"Dataset shape: {X.shape}\")\n",
        "print(f\"Labels shape: {y_true.shape}\")\n",
        "print(f\"Positive cases (diabetes): {y_true.sum()}/{len(y_true)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initialization and activations\n",
        "\n",
        "We initialize the weights for a simple neural network with one hidden layer using small random values.  \n",
        "We also define the activation functions:\n",
        "- **ReLU** for the hidden layer (introduces non-linearity)\n",
        "- **Sigmoid** for the output (gives us probabilities between 0 and 1)"
      ],
      "metadata": {
        "id": "VzOJ-6iV4XvH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize weights with small random values\n",
        "def init_weights(input_dim, hidden_dim):\n",
        "    W1 = np.random.randn(input_dim, hidden_dim) * 0.01  # Small random weights\n",
        "    b1 = np.zeros((1, hidden_dim))                      # Biases start at zero\n",
        "    W2 = np.random.randn(hidden_dim, 1) * 0.01\n",
        "    b2 = np.zeros((1, 1))\n",
        "    return W1, b1, W2, b2\n",
        "\n",
        "# Activation functions\n",
        "def relu(Z):\n",
        "    \"\"\"ReLU activation: max(0, Z)\"\"\"\n",
        "    return np.maximum(0, Z)\n",
        "\n",
        "def sigmoid(Z):\n",
        "    \"\"\"Sigmoid activation: 1 / (1 + exp(-Z))\"\"\"\n",
        "    return 1 / (1 + np.exp(-np.clip(Z, -250, 250)))  # Clip to prevent overflow"
      ],
      "metadata": {
        "id": "XO9GqW-U4fwM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Your task: Implement the training loop\n",
        "\n",
        "Now let's implement the training loop. We'll perform the following steps:\n",
        "\n",
        "1. **Forward pass**: compute predictions by passing data through the network\n",
        "2. **Compute loss**: measure how far off our predictions are using binary cross-entropy\n",
        "3. **Backpropagation**: compute gradients (how much each weight should change)\n",
        "4. **Update weights**: adjust weights in the direction that reduces loss\n",
        "\n",
        "Fill in the missing code below:"
      ],
      "metadata": {
        "id": "yzCSzEUh43aP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(X, y, hidden_dim=4, epochs=500, lr=0.01):\n",
        "    W1, b1, W2, b2 = init_weights(X.shape[1], hidden_dim)\n",
        "    m = X.shape[0]  # Number of training examples\n",
        "    losses = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # === 1. Forward pass ===\n",
        "        Z1 = # Your code here: Linear transform for hidden layer (X @ W1 + b1)\n",
        "        A1 = # Your code here: Apply ReLU activation\n",
        "        Z2 = # Your code here: Linear transform for output (A1 @ W2 + b2)\n",
        "        A2 = # Your code here: Apply sigmoid activation\n",
        "\n",
        "        # === 2. Compute loss ===\n",
        "        # Binary cross-entropy: -mean(y*log(pred) + (1-y)*log(1-pred))\n",
        "        loss = # Your code here: binary cross-entropy loss\n",
        "        losses.append(loss)\n",
        "\n",
        "        # === 3. Backpropagation ===\n",
        "        # Output layer gradients\n",
        "        dZ2 = # Your code here: A2 - y (derivative of loss w.r.t. Z2)\n",
        "        dW2 = # Your code here: A1.T @ dZ2 / m (gradient for W2)\n",
        "        db2 = # Your code here: mean of dZ2 (gradient for b2)\n",
        "\n",
        "        # Hidden layer gradients\n",
        "        dA1 = # Your code here: dZ2 @ W2.T (backprop into hidden layer)\n",
        "        dZ1 = # Your code here: dA1 * (Z1 > 0) (derivative through ReLU)\n",
        "        dW1 = # Your code here: X.T @ dZ1 / m (gradient for W1)\n",
        "        db1 = # Your code here: mean of dZ1 (gradient for b1)\n",
        "\n",
        "        # === 4. Update weights ===\n",
        "        W1 -= lr * dW1\n",
        "        b1 -= lr * db1\n",
        "        W2 -= lr * dW2\n",
        "        b2 -= lr * db2\n",
        "\n",
        "        # Print progress every 50 epochs\n",
        "        if epoch % 50 == 0:\n",
        "            print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
        "\n",
        "    return W1, b1, W2, b2, losses"
      ],
      "metadata": {
        "id": "Nb-HXdOe44B9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Unit test\n",
        "\n",
        "Let's run the training loop and check that:\n",
        "1. The function returns the expected outputs\n",
        "2. The loss decreases over time (indicating learning)"
      ],
      "metadata": {
        "id": "htYR2T7n5FbE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Unit test for training loop\n",
        "print(\"Running unit test for your training loop...\\n\")\n",
        "\n",
        "# Run learner's train() implementation\n",
        "try:\n",
        "    W1, b1, W2, b2, losses = train(X, y_true, epochs=100, lr=0.05)\n",
        "except Exception as e:\n",
        "    raise RuntimeError(f\"Error during training execution: {e}\")\n",
        "\n",
        "# Check that losses is a list of numbers\n",
        "assert isinstance(losses, list), \"Expected 'losses' to be a list.\"\n",
        "assert all(isinstance(loss, (int, float)) for loss in losses), \"All loss values should be numeric.\"\n",
        "\n",
        "# Check loss length\n",
        "assert len(losses) == 100, f\"Expected 100 loss values (one per epoch), but got {len(losses)}.\"\n",
        "\n",
        "# Check that loss decreases\n",
        "if losses[-1] >= losses[0]:\n",
        "    raise AssertionError(f\"Final loss ({losses[-1]:.4f}) is not lower than initial loss ({losses[0]:.4f}). Model may not be learning.\")\n",
        "\n",
        "# Optional: check parameter update\n",
        "if np.allclose(W1, 0) and np.allclose(W2, 0):\n",
        "    raise AssertionError(\"Model weights appear to be unchanged. Check if you're updating them correctly.\")\n",
        "\n",
        "print(\"All tests passed!\")\n",
        "print(f\"Initial loss: {losses[0]:.4f}\")\n",
        "print(f\"Final loss: {losses[-1]:.4f}\")\n",
        "print(f\"Loss reduction: {((losses[0] - losses[-1]) / losses[0] * 100):.1f}%\")"
      ],
      "metadata": {
        "id": "eOlti_vF5Lrr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualize the loss curve\n",
        "\n",
        "Plotting the loss helps us see if the model is learning effectively. A good training curve should show:\n",
        "- **Decreasing loss** over time\n",
        "- **Smooth convergence** (not too noisy)\n",
        "- **Leveling off** when the model has learned as much as it can"
      ],
      "metadata": {
        "id": "visualization_section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train for more epochs to see the full learning curve\n",
        "print(\"Training for 300 epochs...\")\n",
        "W1, b1, W2, b2, losses = train(X, y_true, epochs=300, lr=0.05)\n",
        "\n",
        "# Plot the loss curve\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(losses, linewidth=2)\n",
        "plt.xlabel('Epoch', fontsize=12)\n",
        "plt.ylabel('Binary Cross-Entropy Loss', fontsize=12)\n",
        "plt.title('Training Loss Curve', fontsize=14)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "print(f\"Final training accuracy: {((sigmoid(relu(X @ W1 + b1) @ W2 + b2) > 0.5) == y_true).mean():.1%}\")"
      ],
      "metadata": {
        "id": "visualization_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model evaluation\n",
        "\n",
        "Let's evaluate how well our trained model performs on the training data."
      ],
      "metadata": {
        "id": "evaluation_section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions with the trained model\n",
        "def predict(X, W1, b1, W2, b2):\n",
        "    Z1 = X @ W1 + b1\n",
        "    A1 = relu(Z1)\n",
        "    Z2 = A1 @ W2 + b2\n",
        "    A2 = sigmoid(Z2)\n",
        "    return A2\n",
        "\n",
        "# Get predictions and convert to binary\n",
        "predictions_prob = predict(X, W1, b1, W2, b2)\n",
        "predictions_binary = (predictions_prob > 0.5).astype(int)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = (predictions_binary == y_true).mean()\n",
        "print(f\"Training Accuracy: {accuracy:.1%}\")\n",
        "\n",
        "# Show some example predictions\n",
        "print(\"\\nSample Predictions:\")\n",
        "print(\"Probability | Predicted | Actual\")\n",
        "print(\"-\" * 35)\n",
        "for i in range(10):\n",
        "    prob = predictions_prob[i, 0]\n",
        "    pred = predictions_binary[i, 0]\n",
        "    actual = y_true[i, 0]\n",
        "    print(f\"   {prob:.3f}    |     {pred}     |   {actual}\")"
      ],
      "metadata": {
        "id": "evaluation_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results and next steps:\n",
        "\n",
        "**Congratulations!** You've successfully implemented a neural network training loop from scratch.\n",
        "\n",
        "## What you learned:\n",
        "- How to implement forward propagation\n",
        "- How to compute binary cross-entropy loss\n",
        "- How to perform backpropagation to compute gradients\n",
        "- How to update weights using gradient descent\n",
        "\n",
        "## Experiment with these ideas:\n",
        "- **Change the learning rate**: Try `lr=0.01` or `lr=0.1`. What happens?\n",
        "- **Adjust hidden units**: Try `hidden_dim=8` or `hidden_dim=2`. How does it affect performance?\n",
        "- **More epochs**: Train for 1000 epochs. Does the loss keep decreasing?\n",
        "- **Different initialization**: Try larger initial weights (`* 0.1` instead of `* 0.01`)\n",
        "\n",
        "## Challenge problems:\n",
        "1. **Add another hidden layer** - Can you modify the code to have 2 hidden layers?\n",
        "2. **Try different activation functions** - What about using `tanh` instead of ReLU?\n",
        "3. **Add regularization** - Can you add L2 regularization to prevent overfitting?\n",
        "4. **Implement momentum** - Modify the weight updates to include momentum.\n",
        "\n",
        "Great work! You now understand the fundamentals of how neural networks learn."
      ],
      "metadata": {
        "id": "results_section"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instructor-Only Solution\n",
        "> The following cell contains the complete, correct implementation of the training loop.\n",
        "> In production this would be hidden from learners."
      ],
      "metadata": {
        "id": "nRUr8x2r5hw0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Full solution to training loop\n",
        "def train(X, y, hidden_dim=4, epochs=500, lr=0.01):\n",
        "    W1, b1, W2, b2 = init_weights(X.shape[1], hidden_dim)\n",
        "    m = X.shape[0]\n",
        "    losses = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # === Forward pass ===\n",
        "        Z1 = X @ W1 + b1                # Linear transform for hidden layer\n",
        "        A1 = relu(Z1)                   # ReLU activation\n",
        "        Z2 = A1 @ W2 + b2               # Linear transform for output\n",
        "        A2 = sigmoid(Z2)                # Sigmoid activation (output probability)\n",
        "\n",
        "        # === Compute loss ===\n",
        "        loss = -np.mean(y * np.log(A2 + 1e-8) + (1 - y) * np.log(1 - A2 + 1e-8))  # Binary cross-entropy\n",
        "        losses.append(loss)\n",
        "\n",
        "        # === Backpropagation ===\n",
        "        dZ2 = A2 - y                    # Derivative of loss w.r.t. Z2\n",
        "        dW2 = A1.T @ dZ2 / m            # Gradient for W2\n",
        "        db2 = np.sum(dZ2, axis=0, keepdims=True) / m  # Gradient for b2\n",
        "\n",
        "        dA1 = dZ2 @ W2.T                # Backprop into hidden layer\n",
        "        dZ1 = dA1 * (Z1 > 0)            # Derivative through ReLU\n",
        "        dW1 = X.T @ dZ1 / m             # Gradient for W1\n",
        "        db1 = np.sum(dZ1, axis=0, keepdims=True) / m  # Gradient for b1\n",
        "\n",
        "        # === Update weights ===\n",
        "        W1 -= lr * dW1\n",
        "        b1 -= lr * db1\n",
        "        W2 -= lr * dW2\n",
        "        b2 -= lr * db2\n",
        "\n",
        "        if epoch % 50 == 0:\n",
        "            print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
        "\n",
        "    return W1, b1, W2, b2, losses"
      ],
      "metadata": {
        "id": "KFhe5ZTC5jpA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}