---
# Predicting Diabetes with a Simple Neural Network

This notebook will guide you through implementing a training loop for a simple binary classification task using NumPy.

---

## ðŸ“˜ Instructions

You are given synthetic patient data (BMI, Age, Glucose) and a neural network scaffold.

Complete the training loop so that the model can predict whether a patient has diabetes (1) or not (0).

Youâ€™ll implement:
- Forward pass
- Loss computation
- Backpropagation
- Weight updates

Let's begin!

---

## ðŸ§ª Simulated Dataset and Helper Code

```python
import numpy as np
np.random.seed(42)

# Simulated input data (100 samples, 3 features)
X = np.random.randn(100, 3)
true_weights = np.array([[2], [-1], [1.5]])
y_true = (1 / (1 + np.exp(-X @ true_weights)) > 0.5).astype(int)

# Normalize features
X = (X - X.mean(axis=0)) / X.std(axis=0)
```

---

## ðŸ§  Model Initialization & Activation Functions

```python
def init_weights(input_dim, hidden_dim):
    W1 = np.random.randn(input_dim, hidden_dim) * 0.01
    b1 = np.zeros((1, hidden_dim))
    W2 = np.random.randn(hidden_dim, 1) * 0.01
    b2 = np.zeros((1, 1))
    return W1, b1, W2, b2

def relu(Z):
    return np.maximum(0, Z)

def sigmoid(Z):
    return 1 / (1 + np.exp(-Z))
```

---

## âœï¸ Your Task: Implement the Training Loop

```python
def train(X, y, hidden_dim=4, epochs=500, lr=0.01):
    W1, b1, W2, b2 = init_weights(X.shape[1], hidden_dim)
    m = X.shape[0]
    losses = []

    for epoch in range(epochs):
        # Forward pass
        Z1 = X @ W1 + b1
        A1 = relu(Z1)
        Z2 = A1 @ W2 + b2
        A2 = sigmoid(Z2)

        # Binary cross-entropy loss
        loss = -np.mean(y * np.log(A2 + 1e-8) + (1 - y) * np.log(1 - A2 + 1e-8))
        losses.append(loss)

        # Backpropagation
        dZ2 = A2 - y
        dW2 = A1.T @ dZ2 / m
        db2 = np.sum(dZ2, axis=0, keepdims=True) / m

        dA1 = dZ2 @ W2.T
        dZ1 = dA1 * (Z1 > 0)
        dW1 = X.T @ dZ1 / m
        db1 = np.sum(dZ1, axis=0, keepdims=True) / m

        # Update weights
        W1 -= lr * dW1
        b1 -= lr * db1
        W2 -= lr * dW2
        b2 -= lr * db2

        if epoch % 50 == 0:
            print(f"Epoch {epoch}, Loss: {loss:.4f}")

    return W1, b1, W2, b2, losses
```

---

## âœ… Unit Test: Check the Learnerâ€™s Code

```python
# Run the test
W1, b1, W2, b2, losses = train(X, y_true, epochs=100, lr=0.05)

assert isinstance(losses, list), "Losses should be returned as a list"
assert losses[-1] < losses[0], "Loss should decrease over training"

print("âœ… Training loop runs and loss decreases!")
```

---

## ðŸ” Solution (Instructor-only)

```python
# The learner's function above includes the solution
# For evaluation purposes only
```

---

## ðŸŽ‰ Done!
You're ready to move on. You implemented forward and backward passes, loss tracking, and weight updates.
